apiVersion: v1
kind: Namespace
metadata:
  name: smarthome-pro
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/enforce-version: latest
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: smarthome-config
  namespace: smarthome-pro
data:
  NODE_ENV: "production"
  TZ: "Europe/Stockholm"
  LOG_LEVEL: "info"
  ENABLE_RATE_LIMITING: "true"
  MAX_REQUESTS_PER_MINUTE: "100"
  ENABLE_METRICS: "true"
  # Set REDIS_URL to enable Redis-backed rate limiting across replicas.
  # Example: "redis://redis:6379"  (omit or leave empty to use in-memory store)
  # REDIS_URL: ""
---
# SealedSecret — managed by bitnami/sealed-secrets controller.
# To update secrets, run:
#   kubectl create secret generic smarthome-secrets \
#     --from-literal=HOMEY_TOKEN=<value> \
#     --from-literal=JWT_SECRET=<value> \
#     --dry-run=client -o yaml \
#     | kubeseal --controller-namespace kube-system \
#                --controller-name sealed-secrets \
#                --format yaml > k8s/sealedsecret-smarthome.yaml
# Then apply: kubectl apply -f k8s/sealedsecret-smarthome.yaml
# SealedSecret — application credentials, managed by bitnami/sealed-secrets controller.
# To generate sealed values:
#   kubectl create secret generic smarthome-secrets \
#     --from-literal=HOMEY_TOKEN=<value> \
#     --from-literal=JWT_SECRET=<value> \
#     --dry-run=client -o yaml \
#     | kubeseal --controller-namespace kube-system \
#                --controller-name sealed-secrets \
#                --format yaml
# Paste the resulting encryptedData values below, then apply this file.
# NEVER commit plaintext secret values — only the kubeseal ciphertext is safe to store.
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: smarthome-secrets
  namespace: smarthome-pro
  annotations:
    sealedsecrets.bitnami.com/managed: "true"
spec:
  encryptedData:
    # Replace with ciphertext from kubeseal (cluster-specific, non-transferable).
    HOMEY_TOKEN: "REPLACE_WITH_KUBESEAL_ENCRYPTED_VALUE"
    JWT_SECRET: "REPLACE_WITH_KUBESEAL_ENCRYPTED_VALUE"
  template:
    metadata:
      name: smarthome-secrets
      namespace: smarthome-pro
    type: Opaque
---
# SealedSecret — Alertmanager notification credentials.
# To generate sealed values:
#   kubectl create secret generic alertmanager-secrets \
#     --from-literal=smtp_password=<smtp-password> \
#     --from-literal=webhook_token=<webhook-bearer-token> \
#     --dry-run=client -o yaml \
#     | kubeseal --controller-namespace kube-system \
#                --controller-name sealed-secrets \
#                --format yaml
# Paste the resulting encryptedData values below, then apply this file.
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: alertmanager-secrets
  namespace: smarthome-pro
  annotations:
    sealedsecrets.bitnami.com/managed: "true"
spec:
  encryptedData:
    # smtp_password is mounted as a file at /etc/alertmanager/secrets/smtp_password
    smtp_password: "REPLACE_WITH_KUBESEAL_ENCRYPTED_VALUE"
    # webhook_token is mounted as a file at /etc/alertmanager/secrets/webhook_token
    webhook_token: "REPLACE_WITH_KUBESEAL_ENCRYPTED_VALUE"
  template:
    metadata:
      name: alertmanager-secrets
      namespace: smarthome-pro
    type: Opaque
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: smarthomepro
  namespace: smarthome-pro
  labels:
    app: smarthomepro
    component: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: smarthomepro
  template:
    metadata:
      labels:
        app: smarthomepro
        component: backend
    spec:
      serviceAccountName: smarthome-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
      # Prefer spreading backend replicas across different nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: smarthomepro
              topologyKey: kubernetes.io/hostname
      containers:
      - name: smarthomepro
        image: ghcr.io/ashsolei/smarthomepro:3.3.0
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        ports:
        - containerPort: 3000
          name: http
        - containerPort: 9090
          name: metrics
        envFrom: # nosonar(kubernetes:S6907) — shared ConfigMap intentionally referenced by both deployments
        - configMapRef:
            name: smarthome-config
        - secretRef:
            name: smarthome-secrets
        env:
        - name: PORT
          value: "3000"
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
            ephemeral-storage: "256Mi"
          limits:
            memory: "768Mi"
            cpu: "1500m"
            ephemeral-storage: "512Mi"
        # Startup probe — allows up to 5 min (30 × 10s) for the container to become live
        startupProbe:
          httpGet:
            path: /health
            port: 3000
          failureThreshold: 30
          periodSeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - name: data
          mountPath: /app/data
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: smarthomepro-data
      - name: tmp
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: smarthomepro
  namespace: smarthome-pro
  labels:
    app: smarthomepro
spec:
  type: ClusterIP
  ports:
  - port: 3000
    targetPort: 3000
    name: http
  - port: 9090
    targetPort: 9090
    name: metrics
  selector:
    app: smarthomepro
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dashboard
  namespace: smarthome-pro
  labels:
    app: dashboard
    component: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dashboard
  template:
    metadata:
      labels:
        app: dashboard
        component: frontend
    spec:
      serviceAccountName: smarthome-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
      # Prefer spreading dashboard replicas across different nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: dashboard
              topologyKey: kubernetes.io/hostname
      containers:
      - name: dashboard
        image: ghcr.io/ashsolei/dashboard:3.3.0
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        ports:
        - containerPort: 3001
          name: http
        envFrom: # nosonar(kubernetes:S6907) — shared ConfigMap intentionally referenced by both deployments
        - configMapRef:
            name: smarthome-config
        - secretRef:
            name: smarthome-secrets
        env:
        - name: PORT
          value: "3001"
        - name: HOMEY_URL
          value: "http://smarthomepro:3000"
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
            ephemeral-storage: "128Mi"
          limits:
            memory: "256Mi"
            cpu: "500m"
            ephemeral-storage: "256Mi"
        # Startup probe — allows up to 5 min (30 × 10s) for the container to become live
        startupProbe:
          httpGet:
            path: /health
            port: 3001
          failureThreshold: 30
          periodSeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 3001
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 3001
          initialDelaySeconds: 15
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - name: data
          mountPath: /app/data
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: dashboard-data
      - name: tmp
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: dashboard
  namespace: smarthome-pro
  labels:
    app: dashboard
spec:
  type: ClusterIP
  ports:
  - port: 3001
    targetPort: 3001
    name: http
  selector:
    app: dashboard
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: smarthomepro-data
  namespace: smarthome-pro
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dashboard-data
  namespace: smarthome-pro
  # NOTE: ReadWriteMany requires a CSI driver that supports RWX
  # (e.g. AWS EFS, NFS, CephFS). Verify your storage class supports this mode.
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: smarthome-ingress
  namespace: smarthome-pro
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/use-regex: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - smarthome.yourdomain.com
    secretName: smarthome-tls
  rules:
  - host: smarthome.yourdomain.com
    http:
      paths:
      - path: /(api/.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: smarthomepro
            port:
              number: 3000
      - path: /(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: dashboard
            port:
              number: 3001
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: smarthomepro-hpa
  namespace: smarthome-pro
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: smarthomepro
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # Custom metric — scale on per-pod request rate (requires Prometheus Adapter or KEDA)
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dashboard-hpa
  namespace: smarthome-pro
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dashboard
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
---
# Default deny all traffic in namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: smarthome-pro
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Allow ingress to backend on port 3000 from nginx/ingress only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-backend-ingress
  namespace: smarthome-pro
spec:
  podSelector:
    matchLabels:
      app: smarthomepro
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
        - podSelector:
            matchLabels:
              app: dashboard
      ports:
        - protocol: TCP
          port: 3000
  egress:
    - ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 80
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
---
# Allow ingress to dashboard on port 3001 from nginx/ingress only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dashboard-ingress
  namespace: smarthome-pro
spec:
  podSelector:
    matchLabels:
      app: dashboard
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
      ports:
        - protocol: TCP
          port: 3001
  egress:
    - ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 80
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    - to:
        - podSelector:
            matchLabels:
              app: smarthomepro
      ports:
        - protocol: TCP
          port: 3000
---
# RBAC: ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: smarthome-sa
  namespace: smarthome-pro
---
# RBAC: Role with minimal permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: smarthome-role
  namespace: smarthome-pro
rules:
  - apiGroups: [""]
    resources: ["pods", "configmaps"]
    verbs: ["get", "list"]
---
# RBAC: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: smarthome-rolebinding
  namespace: smarthome-pro
subjects:
  - kind: ServiceAccount
    name: smarthome-sa
    namespace: smarthome-pro
roleRef:
  kind: Role
  name: smarthome-role
  apiGroup: rbac.authorization.k8s.io
---
# ResourceQuota — cap total resource consumption in the smarthome-pro namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: smarthome-quota
  namespace: smarthome-pro
spec:
  hard:
    requests.cpu: "4"
    requests.memory: "4Gi"
    limits.cpu: "8"
    limits.memory: "8Gi"
    pods: "20"
    services: "10"
    secrets: "20"
    configmaps: "20"
---
# PodDisruptionBudget — ensure at least 1 backend replica stays available during disruptions
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: smarthomepro-pdb
  namespace: smarthome-pro
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: smarthomepro
---
# PodDisruptionBudget — ensure at least 1 dashboard replica stays available during disruptions
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: dashboard-pdb
  namespace: smarthome-pro
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: dashboard
---
# ConfigMap — Alertmanager configuration, sourced from monitoring/alertmanager.yml.
# Update receiver URLs and email addresses before deploying to production.
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: smarthome-pro
  labels:
    app: alertmanager
    component: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      smtp_smarthost: 'smtp.yourdomain.com:587'
      smtp_from: 'alertmanager@yourdomain.com'
      smtp_auth_username: 'alertmanager@yourdomain.com'
      smtp_auth_password_file: /etc/alertmanager/secrets/smtp_password
      smtp_require_tls: true

    route:
      group_by: ['alertname', 'job', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: 'default-receiver'

      routes:
        - matchers:
            - severity = "critical"
          receiver: 'critical-receiver'
          group_wait: 10s
          repeat_interval: 1h
          continue: false

        - matchers:
            - severity = "warning"
          receiver: 'warning-receiver'
          group_wait: 30s
          repeat_interval: 4h
          continue: false

    inhibit_rules:
      - source_matchers:
          - severity = "critical"
        target_matchers:
          - severity = "warning"
        equal: ['alertname', 'job', 'instance']

    receivers:
      - name: 'default-receiver'
        email_configs:
          - to: 'ops@yourdomain.com'
            send_resolved: true
            headers:
              Subject: '[SmartHome Pro] {{ .Status | toUpper }} {{ .GroupLabels.alertname }}'

      - name: 'critical-receiver'
        email_configs:
          - to: 'ops@yourdomain.com'
            send_resolved: true
            headers:
              Subject: '[SmartHome Pro] CRITICAL {{ .GroupLabels.alertname }}'
        webhook_configs:
          - url: 'http://your-webhook-endpoint/alerts'
            send_resolved: true

      - name: 'warning-receiver'
        email_configs:
          - to: 'ops@yourdomain.com'
            send_resolved: true
            headers:
              Subject: '[SmartHome Pro] WARNING {{ .GroupLabels.alertname }}'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: smarthome-pro
  labels:
    app: alertmanager
    component: monitoring
spec:
  # Single replica — Alertmanager clustering requires additional peer config.
  # For HA, scale to 3+ and add --cluster.peer flags.
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
        component: monitoring
    spec:
      serviceAccountName: smarthome-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534  # nobody
        runAsGroup: 65534
        fsGroup: 65534
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.27.0
        imagePullPolicy: IfNotPresent
        args:
          - '--config.file=/etc/alertmanager/alertmanager.yml'
          - '--storage.path=/alertmanager'
          - '--web.listen-address=:9093'
          - '--log.level=info'
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        ports:
        - containerPort: 9093
          name: http
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
            ephemeral-storage: "64Mi"
          limits:
            memory: "128Mi"
            cpu: "200m"
            ephemeral-storage: "128Mi"
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9093
          initialDelaySeconds: 15
          periodSeconds: 20
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9093
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
          readOnly: true
        - name: secrets
          mountPath: /etc/alertmanager/secrets
          readOnly: true
        - name: storage
          mountPath: /alertmanager
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: config
        configMap:
          name: alertmanager-config
      - name: secrets
        secret:
          secretName: alertmanager-secrets
          items:
          - key: smtp_password
            path: smtp_password
            mode: 0400
          - key: webhook_token
            path: webhook_token
            mode: 0400
      - name: storage
        emptyDir: {}
      - name: tmp
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: smarthome-pro
  labels:
    app: alertmanager
    component: monitoring
spec:
  type: ClusterIP
  ports:
  - port: 9093
    targetPort: 9093
    name: http
  selector:
    app: alertmanager
---
# Allow Prometheus to reach Alertmanager on port 9093
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-prometheus-to-alertmanager
  namespace: smarthome-pro
spec:
  podSelector:
    matchLabels:
      app: alertmanager
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9093
  egress:
    # Alertmanager needs outbound access for SMTP and webhook delivery.
    - ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 587
        - protocol: TCP
          port: 80
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
