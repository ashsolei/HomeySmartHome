apiVersion: v1
kind: Namespace
metadata:
  name: smarthome-pro
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: smarthome-config
  namespace: smarthome-pro
data:
  NODE_ENV: "production"
  TZ: "Europe/Stockholm"
  LOG_LEVEL: "info"
  ENABLE_RATE_LIMITING: "true"
  MAX_REQUESTS_PER_MINUTE: "100"
  ENABLE_METRICS: "true"
---
# SealedSecret — managed by bitnami/sealed-secrets controller.
# To update secrets, run:
#   kubectl create secret generic smarthome-secrets \
#     --from-literal=HOMEY_TOKEN=<value> \
#     --from-literal=JWT_SECRET=<value> \
#     --dry-run=client -o yaml \
#     | kubeseal --controller-namespace kube-system \
#                --controller-name sealed-secrets \
#                --format yaml > k8s/sealedsecret-smarthome.yaml
# Then apply: kubectl apply -f k8s/sealedsecret-smarthome.yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: smarthome-secrets
  namespace: smarthome-pro
  annotations:
    sealedsecrets.bitnami.com/managed: "true"
spec:
  encryptedData:
    # Replace these placeholder values with actual sealed ciphertext produced by kubeseal.
    HOMEY_TOKEN: "REPLACE_WITH_KUBESEAL_ENCRYPTED_VALUE"
    JWT_SECRET: "REPLACE_WITH_KUBESEAL_ENCRYPTED_VALUE"
  template:
    metadata:
      name: smarthome-secrets
      namespace: smarthome-pro
    type: Opaque
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: smarthomepro
  namespace: smarthome-pro
  labels:
    app: smarthomepro
    component: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: smarthomepro
  template:
    metadata:
      labels:
        app: smarthomepro
        component: backend
    spec:
      serviceAccountName: smarthome-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
      # Prefer spreading backend replicas across different nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: smarthomepro
              topologyKey: kubernetes.io/hostname
      containers:
      - name: smarthomepro
        image: ghcr.io/your-repo/smarthomepro:3.3.0
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        ports:
        - containerPort: 3000
          name: http
        - containerPort: 9090
          name: metrics
        envFrom: # nosonar(kubernetes:S6907) — shared ConfigMap intentionally referenced by both deployments
        - configMapRef:
            name: smarthome-config
        - secretRef:
            name: smarthome-secrets
        env:
        - name: PORT
          value: "3000"
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
            ephemeral-storage: "256Mi"
          limits:
            memory: "768Mi"
            cpu: "1500m"
            ephemeral-storage: "512Mi"
        # Startup probe — allows up to 5 min (30 × 10s) for the container to become live
        startupProbe:
          httpGet:
            path: /health
            port: 3000
          failureThreshold: 30
          periodSeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - name: data
          mountPath: /app/data
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: smarthomepro-data
      - name: tmp
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: smarthomepro
  namespace: smarthome-pro
  labels:
    app: smarthomepro
spec:
  type: ClusterIP
  ports:
  - port: 3000
    targetPort: 3000
    name: http
  - port: 9090
    targetPort: 9090
    name: metrics
  selector:
    app: smarthomepro
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dashboard
  namespace: smarthome-pro
  labels:
    app: dashboard
    component: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dashboard
  template:
    metadata:
      labels:
        app: dashboard
        component: frontend
    spec:
      serviceAccountName: smarthome-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
      # Prefer spreading dashboard replicas across different nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: dashboard
              topologyKey: kubernetes.io/hostname
      containers:
      - name: dashboard
        image: ghcr.io/your-repo/dashboard:3.3.0
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        ports:
        - containerPort: 3001
          name: http
        envFrom: # nosonar(kubernetes:S6907) — shared ConfigMap intentionally referenced by both deployments
        - configMapRef:
            name: smarthome-config
        - secretRef:
            name: smarthome-secrets
        env:
        - name: PORT
          value: "3001"
        - name: HOMEY_URL
          value: "http://smarthomepro:3000"
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
            ephemeral-storage: "128Mi"
          limits:
            memory: "256Mi"
            cpu: "500m"
            ephemeral-storage: "256Mi"
        # Startup probe — allows up to 5 min (30 × 10s) for the container to become live
        startupProbe:
          httpGet:
            path: /health
            port: 3001
          failureThreshold: 30
          periodSeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 3001
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 3001
          initialDelaySeconds: 15
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - name: data
          mountPath: /app/data
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: dashboard-data
      - name: tmp
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: dashboard
  namespace: smarthome-pro
  labels:
    app: dashboard
spec:
  type: ClusterIP
  ports:
  - port: 3001
    targetPort: 3001
    name: http
  selector:
    app: dashboard
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: smarthomepro-data
  namespace: smarthome-pro
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dashboard-data
  namespace: smarthome-pro
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: smarthome-ingress
  namespace: smarthome-pro
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - smarthome.yourdomain.com
    secretName: smarthome-tls
  rules:
  - host: smarthome.yourdomain.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: smarthomepro
            port:
              number: 3000
      - path: /
        pathType: Prefix
        backend:
          service:
            name: dashboard
            port:
              number: 3001
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: smarthomepro-hpa
  namespace: smarthome-pro
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: smarthomepro
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # Custom metric — scale on per-pod request rate (requires Prometheus Adapter or KEDA)
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dashboard-hpa
  namespace: smarthome-pro
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dashboard
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
---
# Default deny all traffic in namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: smarthome-pro
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Allow ingress to backend on port 3000 from nginx/ingress only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-backend-ingress
  namespace: smarthome-pro
spec:
  podSelector:
    matchLabels:
      app: smarthomepro
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
        - podSelector:
            matchLabels:
              app: dashboard
      ports:
        - protocol: TCP
          port: 3000
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
---
# Allow ingress to dashboard on port 3001 from nginx/ingress only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dashboard-ingress
  namespace: smarthome-pro
spec:
  podSelector:
    matchLabels:
      app: dashboard
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
      ports:
        - protocol: TCP
          port: 3001
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    - to:
        - podSelector:
            matchLabels:
              app: smarthomepro
      ports:
        - protocol: TCP
          port: 3000
---
# RBAC: ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: smarthome-sa
  namespace: smarthome-pro
---
# RBAC: Role with minimal permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: smarthome-role
  namespace: smarthome-pro
rules:
  - apiGroups: [""]
    resources: ["pods", "configmaps"]
    verbs: ["get", "list"]
---
# RBAC: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: smarthome-rolebinding
  namespace: smarthome-pro
subjects:
  - kind: ServiceAccount
    name: smarthome-sa
    namespace: smarthome-pro
roleRef:
  kind: Role
  name: smarthome-role
  apiGroup: rbac.authorization.k8s.io
---
# ResourceQuota — cap total resource consumption in the smarthome-pro namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: smarthome-quota
  namespace: smarthome-pro
spec:
  hard:
    requests.cpu: "4"
    requests.memory: "4Gi"
    limits.cpu: "8"
    limits.memory: "8Gi"
    pods: "20"
    services: "10"
    secrets: "20"
    configmaps: "20"
---
# PodDisruptionBudget — ensure at least 1 backend replica stays available during disruptions
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: smarthomepro-pdb
  namespace: smarthome-pro
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: smarthomepro
---
# PodDisruptionBudget — ensure at least 1 dashboard replica stays available during disruptions
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: dashboard-pdb
  namespace: smarthome-pro
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: dashboard
